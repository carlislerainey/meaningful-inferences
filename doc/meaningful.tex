%
\documentclass[12pt]{article}

% The usual packages
\usepackage{fullpage}
\usepackage{breakcites}
\usepackage{setspace}
\usepackage{endnotes}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{rotating}
\usepackage{dcolumn}
\usepackage{longtable}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage[usenames,dvipsnames]{color}
\usepackage{url}
\usepackage{natbib}
\usepackage{framed}
\usepackage{epigraph}
\usepackage{lipsum}
\usepackage[font=small,labelfont=sc]{caption}
\restylefloat{table}
\bibpunct{(}{)}{;}{a}{}{,}

% Set paragraph spacing the way I like
\parskip=0pt
\parindent=20pt

% Define mathematical results
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{claim}{Claim}
\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

% Set up fonts the way I like
\usepackage{tgpagella}
\usepackage[T1]{fontenc}
\usepackage[bitstream-charter]{mathdesign}

%% Set up lists the way I like
%Redefine the first level
\renewcommand{\theenumi}{\arabic{enumi}.}
\renewcommand{\labelenumi}{\theenumi}
%Redefine the second level
\renewcommand{\theenumii}{\alph{enumii}.}
\renewcommand{\labelenumii}{\theenumii}
%Redefine the third level
\renewcommand{\theenumiii}{\roman{enumiii}.}
\renewcommand{\labelenumiii}{\theenumiii}
%Redefine the fourth level
\renewcommand{\theenumiv}{\Alph{enumiv}.}
\renewcommand{\labelenumiv}{\theenumiv}

% Create footnote command so that my name
% has an asterisk rather than a one.
\long\def\symbolfootnote[#1]#2{\begingroup%
\def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}

\hypersetup{
 pdftitle={Meaningful Inferences}, % title
 pdfauthor={Kelly McCaskey and Carlisle Rainey}, % author
 pdfkeywords={hypothesis testing} {confidence intervals} {substantive significance}
 pdfnewwindow=true, % links in new window
 colorlinks=true, % false: boxed links; true: colored links
 linkcolor=red, % color of internal links
 citecolor=red, % color of links to bibliography
 filecolor=red, % color of file links
 urlcolor=red % color of external links
}

% enable comments in pdf
\newcommand{\comment}[1]{\textcolor{blue}{#1}}

\begin{document}

\begin{center}
{\LARGE Meaningful Inferences}\\\vspace{2mm}
{\large \comment{[subtitle here]}\symbolfootnote[1]{We thank [many people]. The analyses presented here were conducted with \texttt{R} 3.1.0 and Stata 13. All data and computer code necessary for replication are available at [GitHub Repositroy].}}\\\vspace{2mm}


\vspace{10mm}

Kelly McCaskey\symbolfootnote[2]{Kelly McCaskey is a Ph.D. student in the Department of Political Science, University at Buffalo, SUNY, 520 Park Hall, Buffalo, NY 14260 (\href{mailto:kellymcc@buffalo.edu}{kellymcc@buffalo.edu}).}

\vspace{3mm}

Carlisle Rainey\symbolfootnote[3]{Carlisle Rainey is Assistant Professor of Political Science, University at Buffalo, SUNY, 520 Park Hall, Buffalo, NY 14260 (\href{mailto:rcrainey@buffalo.edu}{rcrainey@buffalo.edu}).}
\end{center}

% Remove page number from first page
%\thispagestyle{empty}

\vspace{10mm}


% Abstract
{\centerline{\textbf{Abstract}}}
\begin{quote}\noindent [Abstract here.] \end{quote}
\thispagestyle{empty}

\epigraph{Data analysis must use mathematical argument and mathematical results as bases for judgment rather than as bases for proof or stamps of validity.}{Tukey (1962, p. 6)}
%\epigraph{Far better an approximate answer to the \textit{right} question, which is often vague, than an exact answer to the \textit{wrong} question, which can always be made precise.}{Tukey (1962, p. ???)}
%\epigraph{...an investigator would be misled less frequently and would be more likely to obtain the information he seeks were he to formulate his experimental problems in terms of the estimation of population parameters, with the establishment of confidence intervals about the estimated values, rather than in terms of a null hypothesis against all possible alternatives.}{Lyle V. Jones (1955, p.407)}
%\epigraph{Tests of significance are preliminary or ancillary. The emphasis on tests of significance, and the consideration of the results of each experiment in isolation, have had the unfortunate consequence that scientific works have regarded the execution of a test of significance on an experiment as the ultimate objective. Results are significant or not significant and that is the end of it.}{Yates (1951, p. ???)}

%\end{document} % uncomment to make a title-page with author info

\newpage
\doublespace
% Set first page of text as page 1. I don't care for this
%   feature because then the page numbers don't correspond
%   to the pdf pages.
%\setcounter{page}{1}

\section*{Introduction}

[introduction here.]

\section*{What We Want To Know}

Most empirical research in political science focuses on estimating the effect $\Delta$ of an explanatory variable $x$ on the expected value of an outcome of interest $y$. Formally, we might suppose that $E(y | x) = f(x)$ and define the ``effect'' or ``quantity of interest'' $\Delta$ as the difference between the average outcomes when $x$ takes on a substantively meaningful high value and low value, so that $\Delta = E(y | x = x_{hi}) - E(y | x = x_{lo}) = f(x_{hi}) - f(x_{lo})$. Importantly, empirical work usually focuses on answering three fundamental questions about the nature of the relationship between $x$ and $y$. Each of these question requires increasing levels of analysis and substantive interpretation and each receives decreasing levels of attention in political science research.

\subsection*{Establish the Sign}

The first question that question that empirical research usually attempts to answer is the direction of the effect. Is the effect positive or negative? Some research argues theoretically and empirically for ``no effect'' or ``a negligible effect'' (e.g., Kam and Palmer 2008, see Rainey 2014), but most hypotheses posit the direction of an effect. \comment{[We could put one or two nice examples here.]} Suppose, for clarity, that the researcher offers a directional research hypothesis, suggesting that an effect of interest $\Delta$ is positive. Formally, we might denote this hypothesis as $H_r: \Delta > 0$. Then the researcher compares this research hypothesis to a null hypothesis that suggests that the research hypothesis is false, or equivalently, that the effect lies outside the region suggested by the research hypothesis. Formally, we might write this as $H_0: \Delta \leq 0$. To assess the evidence against the null hypothesis, the researcher usually \comment{[How often in our sample?]}calculates a $p$-value, which is the probability of obtaining (hypothetical) data at least as extreme as the observed data if the null hypothesis were true. If this $p$-value is sufficiently small (by convention, less than 0.05), then the researcher rejects the null hypothesis in favor of the research hypothesis. In our example, the researcher would reject negative effects and conclude that the effect is positive. However, if the $p$-value is not sufficiently small, the the researcher declares that the data do not offer compelling evidence against the null and notes that the direction of the effect remains uncertain (though some research incorrectly takes a $p$-value greater than 0.05 as evidence \textit{in favor of} the null hypothesis, see Rainey 2014).

\comment{[We should discuss that some research focuses exclusively on this ``sign-and-significance'' approach.]}

There are three aspects of the sign-and-significance approach make note of. First, this style of argumentation is ubiquitous in political science. It is extremely rare to find empirical research in political science that does not perform a hypothesis test of some sort \comment{[It would be nice if we had some sort of data on this.]}. Second, note that this approach is compelling not because it argues that the observed data are consistent with the researcher's claim, but because the data are inconsistent with other claims. Third, notice that this argument for the direction of the effect explicitly takes into account the uncertainty of the estimated effect. If the uncertainty is large relative to the magnitude of the estimate then the researcher cannot (and usually does not) make confident claims about the direction of the effect of interest. \comment{[We could put a really nice example here of a researcher drawing a cautious conclusion from an insignificant effect.]} However, if the uncertainty is small relative to the size of the estimated effect, then the researcher can draw confident conclusions about the direction of the effect. \comment{[We could put a really nice example here of a researcher drawing a confidence conclusion from a significant effect.]}

\subsection*{Establish the Magnitude}

Yet recent methodological work emphasizes that empirical work should go beyond estimating the direction of effects (King, Tomz, Wittenberg 2000; Hanmer and Kalkan 2013; Gross 2014). In addition to the direction of an effect, its size, or magnitude, matters as well. 

Some statistical models have parameters that a naturally interpretable. For example a simple difference-in-means or normal-linear model have directly interpretable coefficients as long as the scales of the variables are reasonable and the model doesn't include non-linear or product terms. Outside of this atypical case, however, the researcher must do additional work to estimate a substantively meaningful quantity of interest.

In discussing how scholars might interpret a model of the effects of education on income, King, Tomz, and Wittenberg (2000, p. 348) write:

\begin{quote}
Bad interpretations are substantively ambiguous and filled with methodological jargon: `the coefficient on education was statistically significant at the 0.05 level.' Descriptions like this are very common in social science, but students, public officials, and scholars should not need to understand phrases like `coefficient,' `statistically significant,' and `the 0.05 level' to learn from the research. Moreover, even statistically savvy readers should complain that the sentences does not convey the key quantity of interest: how much higher the starting salary would be if the student attended college for an extra year.
\end{quote}

The emphasis on effect magnitude is not a new idea. Commenting on the consequences of Fisher's null hypothesis significance test, Yates (1951) writes: ``[I]t has caused scientific workers to pay undue attention to the results of the tests of significance they perform on their data, particularly data derived from experiments, and too little to the estimates of the magnitude of the effects they are investigating.'' Yates continues: ``Tests of significance are preliminary or ancillary. The emphasis on tests of significance, and the consideration of the results of each experiment in isolation, have had the unfortunate consequence that scientific works have regarded the execution of a test of significance on an experiment as the ultimate objective. Results are significant or not significant and that is the end of it.''

Fortunately, with the recent conceptual work by King (1989); Long (1997); King, Tomz, and Wittenberg (2000); Berry, DeMeritt, and Esarey (2010); and Hanmer and Kalkan (2013), political scientists are moving beyond a simple ``sign-and-significance'' approach to presenting substantively meaningful measures of effect magnitude in addition to making compelling arguments about the direction of effect with hypothesis tests. \comment{[We should add a few examples of scholars providing their readers with substantively meaningful measures of effect size, such as first-differences.]}

\subsection*{Establish the Substantive Importance}

But ultimately, researchers want to move beyond the simple presentation of effect magnitude and make a judgment about the meaningfulness of the effects. Are the effects large or small? Are they substantively important? Are they relevant for policy? Are they scientifically important? Hanmer and Kalkan (2013, p. 264) write: ``[W]e take it as given that understanding whether the relationship is substantively significant, rather than just statistically significant, is the ultimate goal, as it is a necessary part of vaulting one's theory.'' \comment{[We should add a few examples of papers making an argument that there effects are large enough to be important.----We might also want to add some material here about the necessity of substantive judgment. Or it might make sense to save it for later.]}


\section*{Potential Pitfalls in Reasoning About Effect Magnitude}

There are two possible errors that researchers might make when making claims about the substantive magnitude of the effect. The first has been discussed often and will come as no surprise to some readers. The second is more subtle but has negative consequences that are nearly as severe as the first. The first potential mistake that researchers might make is interpreting the p-value as a measure of magnitude and the second is interpreting the magnitude without explicitly accounting for the uncertainty of the estimate. 

\subsection*{Small $p$-Values Do Not Indicate Large Effect}

A $p$-value is simply the probability of observing data at least as extreme as the observed data if the null hypothesis were true. All else equal, the p-value usually gets smaller as the effect size under investigation increases. But the $p$-value is also a function of the sample size, for example. ``Very significant'' results (e.g., $p < 0.001$) might simply mean that the research has a large sample. Even if the researcher finds statistically significant results with a small sample, the small $p$-value only indicates that the effect size is large relative to the uncertainty. It says nothing about the size of the estimate relative to some standard of substantive importance. With a very small p-value (e.g., $p < 0.001$), substantive experts might judge the effect to be large, moderate, small, or even negligible. Experts might call some of these effect important, somewhat important, slightly important, or not at all important. The $p$-value is indirectly related to substantive significance at best. This disconnect occurs for two reasons. First, the $p$-value is only partly determined by the effect size--sample size plays a large part as well. Second, the absolute magnitude of the effect cannot be deemed substantively large, small, or negligible without the judgment of a substantive expert. 

\subsection*{Current Practice Does Not Account for Uncertainty}

The standard scientific practice in political science computes substantively interpretable estimates of effects and (1) determines whether these estimates are statistically significant and, if so, (2) makes a judgement about whether the magnitude is substantively important. \comment{[I need to add a cite here, but perhaps several examples will do.]} This two step procedure has three unfortunate consequences.

\singlespace
\begin{enumerate}
\item This tempts political scientists to treat all results that are not statistically significant similarly, drawing no distinction between large, imprecise estimates and small, precise estimates (see Rainey 2014).
\item This tempts political scientists to treat all statistically significant and substantively large estimates similarly, drawing no distinction between large, imprecise estimates and large, precise estimates.
\item This tempts political scientists to treat ``barely significant,'' large estimates and ``almost significant,'' large estimates quite differently, drawing a strong distinction between two similar estimates with similar uncertainty.
\end{enumerate}
\doublespace

\begin{figure}[h]
\begin{center}
\includegraphics[scale = .6]{figs/example-cis.pdf}
\caption{\comment{[Add caption here.]}}\label{fig:logit}
\end{center}
\end{figure}

\renewcommand{\arraystretch}{1.5}
\begin{table}
\begin{center}
\begin{scriptsize}
\begin{tabular}{|>{\centering\arraybackslash}m{.5in}m{1.75in}>{\centering\arraybackslash}m{1.75in}>{\centering\arraybackslash}
m{1.75in}|}
\hline

Study & Sign and Significance Method & Sign, Significance, Magnitude, and Importance Method & Intuitive Interpretation\\ 
\hline
Study A & ``positive and significant'' & ``positive, significant, and substantively large'' & ``We have strong evidence for a large, substantively meaningful effect.''\\
Study B & ``positive and significant'' & ``positive, significant, and substantively large'' & ``We have only weak evidence for a large, substantively meaningful effect, because the data are also consistent with negligible effects near zero.''\\
Study C & ``not statistically significant'' & ``not statistically significant'' & ``We have only weak evidence for a large, substantively meaningful effect, because the data are also consistent with negligible effects near zero.''\\
Study D & ``positive and significant'' & ``positive and significant, but substantively small' & ``We have strong evidence against a substantively meaningful effect.''\\
Study E & ``not statistically significant'' & ``not statistically significant'' & ``We have strong evidence against a substantively meaningful effect.''\\
\hline
\end{tabular}\caption{\comment{[Add caption here]}}\label{tab:lit}
\end{scriptsize}
\end{center}
\end{table}

The key takeaway point is that it is importance to apply similar standards of evidence to arguments for positive (or negative) effects and arguments for meaningfully positive (or meaningfully negative) effects. The usual logic of hypothesis testing requires that the researcher only declare that an effect is positive if and only if the evidence points overwhelming against negative effects. Similarly, researchers should not declare a positive estimate substantively meaningful simply because it is inconsistent with with negative effects and above some threshold. A better standard of evidence would require that researchers declare an effect meaningful if and only if it is inconsistent with negligible effects.

\comment{[Add a subsection here discussing Rainey (2014) and Gross (2014)]}


\section*{A Compelling Argument for a Meaningful Effect}

Researchers can make much more compelling and transparent arguments for meaningful effects by explicitly testing the claims we are making. For example, if  a researcher claims that an effect is positive and substantively meaningful, then the research hypothesis is not $H_r: \Delta > 0$. Instead, it's $H_r: \Delta > m$, where $m$ represents the researchers judgment about the smallest substantively interesting effect (or the largest substantively negligible effect). There is not need to jettison the entire framework. In fact, the problems identified above can be addressed and the usual hypothesis testing framework can be kept intact if researchers are simply willing to apply their substantive judgment about which effects are and are not meaningful to their hypotheses rather than the estimated magnitude. 

The current practice in political science for arguing for a meaningful effect proceeds as follows:

\singlespace
\begin{enumerate}
\item Hypothesis that the effect is positive (or negative)
\item Test the directional hypothesis.
\item If the null hypothesis can be rejected, then compute a substantively interpretable measure of the estimated effect size and make a substantive judgment about whether the estimate is substantively meaningful or not.
\end{enumerate}
\doublespace
The alternative approach that I propose proceeds as follows:

\singlespace
\begin{enumerate}
\item Hypothesize that the effect is positive and substantively meaning (or negative and substantively meaningful).
\item Make a substantive judgment about the smallest substantively meaningful effect.
\item Test the research hypothesis.
\end{enumerate}
\doublespace

\subsection*{Stating a Research Hypothesis of a Meaningful Effect}

The first step is easy, but important. In order to make compelling arguments for meaningful effects, researcher must clearly state their claims. Rather than simply hypothesizing that an effect is positive or negative, researcher must make their claim that a variable has a meaningful effect from the outset. For example, Levendusky and Horowitz (2012) hypothesize that ``bipartisan support in Congress for the president's policy should decrease audience costs.'' Notice that this hypothesizes a direction, but not a magnitude of the effect. Yet in discussing this hypothesis, Levendusky and Horowitz write: 

\begin{quote}
This type of unexpected (disconfirming) cue has an especially large effect on voters' decision-making processes (Baum and Groeling 2009; Eagly, Wood, and Chaiken 1978). In effect, it sends voters a strong signal that this is not a partisan decision, but rather a decision about what is best for the nation. Further, the fact that even the president's rivals supported his decision suggests to voters that the president did make the right call, which should lead all voters (regardless of partisan affiliation) to punish the president less harshly, thereby minimizing audience costs. [italics ours]
\end{quote}

Thus, Levendusky and Horowitz carefully theorize about the magnitude of the effect, but only include the direction implied by the theory, and not the magnitude, into their hypothesis. They could improve the test of their theory by building the implication about the size of the effect directly into their hypothesis, predicting that ``bipartisan support in Congress for the president's policy should substantially decrease audience costs.'' I have simply added ``substantially'' to their hypothesis. This provides a strong indication to readers (and the researcher) that the theory implies the effect should be large and the researcher will provide a strong argument for a large negative effect, as opposed to simply a negative effect. 


\subsection*{Choosing $m$}

In order to make a compelling argument that an effect is substantively meaningful, the research must carefully define exactly which effects are and are not substantively meaningful. This quantity is the identical concept proposed by Rainey (2014), and as Rainey notes, political scientists should not insist on hard and fast rules for judging the effects that are and are not substantively meaningful (though such rules of thumb have been presented, see Glass 1976 \comment{[add a cite to Cohen here as well]}). Instead, we must insist that substantive scholars making substantive claims about politics also make substantive judgments about the importance of their effects. Thompson (2001) notes for example, that ``if people interpreted effect sizes with the same rigidity that $\alpha = 0.05$ has been used in statistical testing, we would merely be being stupid in another metric.'' Kirk (1996) notes that this judgment is ``influenced by a variety of factors, including the researcher's value system, societal concerns, assessment of costs and benefits, and so on." Despite this element of subjectivity, Thompson (2002) writes: ``[T]he existence of effect size benchmarks should not justify abrogating the responsibility for arguing for effect import in the specific context of a given study. It is not necessary to have universal benchmarks regarding what effect sizes may be deemed noteworthy. The reader with a value system widely different than that of an author might reasonably disagree with the author about whether the effect size is noteworthy and then simply ignore the study.'' 


Formal hypothesis tests and judgments about substantive importance are qualitatively different decisions and have different strengths and weaknesses. Estimation and hypothesis tests are relatively automatic and ``objective,'' but are not at all transparent \comment{[Here we can mention the number of regression models people fit, etc. Add lots of cites to this point.]} Researchers do not fit one model and report the single $p$-value. Instead they fit many models and report the one that ``makes most sense'' in light of their approach, theoretical model, normative concerns, and the results of the model. \comment{[add some cites to the p-hacking literature, such as Justin's paper.]} Substantive judgments about effect sizes, however, require a large initial investment of careful thought to argue that certain effects are or are not substantively important. However, this judgment is quite transparent. Readers are free to reject the author's judgment and substitute their own. Further, ``automatic'' and ``objective'' procedures are not always (or perhaps usually) desirable. Substantive scholars making substantive points about politics should be allowed and encouraged to make substantive judgments about magnitude. Indeed, Kirk (2001 \comment{[check if we can get this from the 1996 paper]}) writes:

\begin{quote}
[R]esearchers have an obligation to make this kind of judgment. No one is in a better position than the researcher who collected and analyzed the data to decide whether the effects are trivial or not. It is a curious anomaly that researchers are trusted to make a variety of complex decisions in the design and execution of an experiment, but in the name of objectivity they are not expected to nor even encouraged to decide whether the effects are practically significant'' (p. 214)
\end{quote}

\section*{Testing a Hypothesis of a Meaningful Effect}

Once the researcher has clearly identified that set of effects that he considers substantively meaningful, the testing problem is straightforward. For an effect of interest $\Delta$, the a researcher positing a ``substantively meaningful, positive effect'' must simply test her research hypothesis $H_r: \Delta > m$ against the null hypothesis $H_0: \Delta \leq m$. For a researcher positing a ``substantively meaningful, negative effect'' must simply test her research hypothesis $H_r: \Delta < -m$ against the null hypothesis $H_0: \Delta \geq -m$.\footnote{Though rarely used in practice, this the way hypothesis testing is introduced in in introductory textbooks.}

The case of $t$-statistics illustrates the parallel between testing for a meaningful positive effect and simply testing for a positive effect. If a researcher simply wishes to argue that an effect is positive (thoough perhaps substantively irrelevant), the $t$-statistic is given by $t = \dfrac{\Delta}{\sqrt{\hat{Var}(\Delta)}}$. If a researchers wish to argue that an effect is positive and substantively meaningful, then the required $t$-statistic is given by $t = \dfrac{\Delta - m}{\sqrt{\hat{Var}(\Delta)}}$. For hypotheses of meaningful negative effects, the required $t$-statistics is given by [something here consistent with notation in a book.]. The researcher can then use these $t$-values to compute $p$-values and determine if the respective null hypothesis of ``no effect'' and ``a negligible effect'' can be rejected.

\subsection*{Confidence Intervals}

While the hypothesis testing framework is sometimes clear and convenient, confidence intervals offer even more information and are easier for readers (and researchers) to interpret. Specifically, the researcher simply needs to check that a 90\% confidence interval contains values that are only consistent with the research hypothesis of a meaningful effect. Therefore, if the 90\% confidence interval contains only large, meaningful effects, then the researcher can confidently reject small, negligible effects. However, if the 90\% confidence contains effects that are inconsistent with the hypothesis of a meaningful effect, such as small, negligible effects, the evidence for the researchers claim is (correctly) identified as weaker. 

A $100(1-\alpha)$\% confidence interval contains the set of values that cannot be rejected by a size-$\alpha$ two-tailed test. Thus, all values $u^{+/-}_{\alpha}$ that fall outside (i.e., above or below) the confidence interval are rejected by a two-tailed test of size $\alpha$. Confidence intervals have a similar relationship with one-tailed tests. All values $u^{-}_{2\alpha}$ that fall \textit{below} a $100(1-2\alpha)$\% are rejected by a one-tailed test of the null hypothesis that the true parameter lies at or below $u^{-}_{2\alpha}$. Similarly, all values $u^{+}_{2\alpha}$ that fall \textit{above} a $100(1-2\alpha)$\% are rejected by a one-tailed test of the null hypothesis that the true parameter lies at or above $u^{+}_{2\alpha}$ (see Achen 1982 and Casella and Berger 2002, pp. 419-423). Thus, there is a one-to-one correspondence between one- and two-tailed hypothesis tests of size $\alpha$ and 90\% and 95\% confidence intervals respectively (see esp. Casella and Berger 2002, pp. 419-423).
Concretely, if the researcher predicts that an effect is positive and finds that the 90\% confidence interval contains only positive effects, this is equivalent to rejecting the null hypothesis that the effect is less than or equal to zero at the 0.05 level.\footnote{Similarly, if the researcher predicts that an effect is \textit{negative} and finds that the 90\% confidence interval contains only \textit{negative} effects, this is equivalent to rejecting the null hypothesis that the effect is \textit{greater} than or equal to zero at the 0.05 level (see Freedman, Pisani, and Purves 2007 pp. 383-385 for more on this point and DeGroot and Schervish 2012 pp. 485-493 for an alternative perspective).}

Thus, confidence intervals partition the parameter space into the effects that are plausible (given the data and the model) and those that are not.\footnote{Some care is warranted in interpreting confidence intervals . It is tempting for researchers to say that ``because the 90\% confidence interval ranges from 0.1 to 0.4, there is a 90\% chance that the true effect falls between 0.1 and 0'' (Hoestra, Morey, Rouder, and Wagenmakers 2014). However, frequentist confidence intervals require the probability statement to be placed on the statistic (in this case, the confidence interval) and not the parameter. Some statisticians therefore prefers the language ``we can be 95\% confidence that the true value lies between 0.1 and 0.4,'' though this requires a technical understanding of the word confident. In most applied settings, though, frequentist confidence intervals and Bayesian credible intervals, which can be interpreted as probability statements about parameters, are nearly identical (though see Berger and Wolpert 1998, ch. 2), rendering the debate somewhat moot.}

Kirk (2001) writes:

\begin{quote}
A confidence interval is just as useful as a null hypothesis significance test for deciding whether chance or sampling variation is an unlikely explanation for an observed effect. Unlike a test statistic for, say, a contrast in means, a point estimate and confidence interval use the same unit of measurement as the data. This facilitates the interpretation of results and makes trivial effects harder to ignore.
\end{quote}

\section*{Replication of Hultman, Kathman, and Shannon (2013)}

\comment{[Add replication here.]}

\section*{Replication of Kam and Zechmeister (2013)}

\comment{[Add replication here]}

\section*{Conclusion}

\comment{[Discuss situations in which the NHST might be better than confidence intervals.]}


[This section needs to be fleshed out a little, but I'm still not quite sure what I want to say yet.]


In this paper, I have argued that political scientists should use confidence intervals when arguing that an effect has a substantively large effect (as opposed to simply arguing that an effect is different from zero). However, I'd like to mention a couple of reasons why confidence intervals and magnitude might remain relevant and mention three potential extensions of my argument.

The usual tests of directional hypotheses sometimes teach us something useful. First, while the usually approach to hypothesis testing has faced ample criticism from methodologists, some researchers defend its merits. Indeed, my own argument for moving beyond simple tests of directional hypotheses assumes that we could be learning more from our data, not that current practice teaches us nothing. \comment{[Discuss the criticisms of Hagan 1997 and Wainer 1999 in sufficient detail.]}

In some situations, the scale of the outcome or explanatory might not be interpretable, making the magnitude of effects difficult to discern. In this case, we researchers are left with two options. First, they can simply rely on directional hypothesis tests. Second, they can rely on standardized measures of effect size.


While the above suggests that there are a least a couple of situations in which the directional hypothesis test remains the best practice, researchers might consider moving beyond my suggestions in many other situations. 

However, perhaps political scientists should use confidence intervals more generally rather than relying extensively on the null hypothesis significance test.

Rather than relying on the prior determination of m and testing the hypothesis that the effect is substantively meaningful, perhaps researchers should simply compute the confidence interval and see what the data have to say. Note that this approach would work regardless of whether the researcher posts a meaningful, directional, or negligible effect (Rainey 2014). Perhaps political scientists should follow Achen's (1982) advice:

\begin{quote}
What general advice can be given for interpreting confidence intervals? The best use of them depends on the problem at hand, and no universal instructions can be given. However, one rarely errs by giving a 95\% interval, explaining what the endpoints would mean substantively if each were true, and interpreting the overall results in such a way as to allow for the possibility that either of those endpoints is, in fact, the truth.
\end{quote}

Jones (1955) suggests that ``an investigator would be misled less frequently and would be more likely to obtain the information he seeks were he to formulate his experimental design problems in terms of the estimation of population parameters, with the establishment of confidence intervals about the estimated values, rather than in terms of a null hypothesis against all possible alternatives. (cited in Tukey 1991)''


Confidence intervals allow researchers to quietly drop the requirement of a pre-determined $m$ if they so choose and rely on a more continuous interpretation of the evidence. Rather, they compute the confidence intervals for the substantively interpretable effects first, and then discuss the substantive impacts of the results.

It it is will known that researchers can construct confidence intervals equivalently that are equivalent to non-directional hypotheses, directional hypotheses, and hypotheses of negligible effects (Rainey 2014). In this paper, I argue that confidence intervals are useful when researchers are arguing for meaningful effects as well. Further, Gross (2014) offers a useful synthesis of my argument combined with Rainey (2014). But confidence intervals allow data to offer ``hint'' in addition to compelling evidence for claims. Kirk (2001) observes that ``the focus should be on what the data tell us about the phenomenon under investigation.'' If the focus is on summarizing the information offered by the data and not on a detailed binary decision, then the data are allowed to make offer researchers ``hints'' even when the data do not offer strong evidence.


\comment{[Add some discussion about hints here.]}



%\singlespace 
%\normalsize
%\singlespace
%\bibliographystyle{apsr_fs}
%\bibliography{bibliography}

\end{document}

%%% Miscellaneous text not (yet?) incorporated into the document


King, Tomz, and Wittenberg (2000, p. 348) write:

``Having estimated the statistical model, many researchers stop after a cursory look at the signs and `statistical significance? of the effect parameters. This approach obviously fails to meet our criteria for meaningful statistical communication since, for many nonlinear models, [the parameters] are difficult to interpret and only indirectly related to the substantive issues that motivated the research. Instead of publishing the effect coefficients and the ancillary parameters, researchers should calculate and present quantities of direct substantive interest.?'

King (1998, p. 102) writes more strongly: ``[I]f [the model coefficients] have has no substantive interpretation for a particular model, the that model should not be estimated. I say this so emphatically because the inordinate number of journal articles that report logit or other coefficients only to eventually describe them as `unintuitive? or even `meaningless.??? 


One criticism of the p-value is that it is misinterpreted in a whole host of ways (e.g., lots of cites). One of these misinterpretations is that statistically significant (and, even more so very significant) results are (1) of large magnitude and (2) substantively important. This logical leap from statistical significant might be explicit, but is more often implicit in that the researcher allows or encourages her readers to make the leap themselves. But as others have made clear, statistical significance does not imply a result is of no substantive importance.

What fewer scholars have recognized, though, is that a statistically significant and substantively meaningful estimate does not imply that we can confidently reject meaningless effects (or alternatively, that we can be confident that the effect is meaningful.) Indeed, it is possible to have an estimate that is substantively large (perhaps a 10\% increase in some applications) and statistically significant (e.g., p < 0.05) that is also consistent with meaningless effects (perhaps a 2\% increase).

Results are never certain, but Tukey (1991) notes that it is crucial for researchers to consider the change of error carefully and clearly commicate the nature and likelihood of the possible errors.

